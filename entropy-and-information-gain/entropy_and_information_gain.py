# -*- coding: utf-8 -*-
"""entropy_and_information_gain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CE6urK4Ysifi6tsM2fkKseomZfTfjOmD

**Загрузка библиотек**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree

"""**Загрузка датасета**"""

date = pd.read_csv("train_data.csv")
date.head()

"""**Подготовка данных**"""

X = date.drop(['num'], axis=1)
y = date['num']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)

X_train = date.drop(['num'], axis=1)
y_train = date['num']

X_train.shape

"""**Обучение модели**"""

model = DecisionTreeClassifier(criterion='entropy')
model.fit(X_train, y_train)

"""**Расчёт entropy и information gain**

Визуализация дерева
"""

plot_tree(model, filled=True)

"""Вывод entropy и samples для левого и правого поддерева"""

l_node = model.tree_.children_left[0] 
n1 = model.tree_.n_node_samples[l_node] 
e1 = model.tree_.impurity[l_node]

r_node = model.tree_.children_right[0] 
n2 = model.tree_.n_node_samples[r_node] 
e2 = model.tree_.impurity[r_node]

"""Расчет information gain для корня дерева"""

IG = 0.996 - (n1*e1 + n2*e2) / (n1 + n2)
IG